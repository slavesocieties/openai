{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a06ec76c",
   "metadata": {},
   "source": [
    "# Data preparation and analysis for chat model fine-tuning\n",
    "\n",
    "This notebook serves as a tool to preprocess and analyze the chat dataset used for fine-tuning a chat model. \n",
    "It checks for format errors, provides basic statistics, and estimates token counts for fine-tuning costs.\n",
    "The method shown here corresponds to the [current fine-tuning method](https://platform.openai.com/docs/guides/fine-tuning) for gpt-3.5-turbo.\n",
    "See [legacy fine-tuning](https://platform.openai.com/docs/guides/legacy-fine-tuning) for models like babbage-002 and davinci-002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e63973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "013bdbc4",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "We first load the chat dataset from an [example JSONL file](https://github.com/openai/openai-cookbook/blob/main/examples/data/toy_chat_fine_tuning.jsonl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c248ccd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 8\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are assisting a historian with a collection of images of early modern Catholic sacramental records written in Spanish. The historian will provide you with images containing a baptismal register handwritten in early modern Spanish and ask you to transcribe the text of the register. Output this transcription as JSON.'}\n",
      "{'role': 'system', 'content': 'Please transcribe the text manually. Do not attempt to use any OCR processes.'}\n",
      "{'role': 'system', 'content': 'The historian will provide you with both color and grayscale versions of the image. Use both versions together to improve the accuracy of your transcription.'}\n",
      "{'role': 'system', 'content': 'Do not use context-based inference to transcribe dates, names of places or people, characteristics of people, or relationships between people. Attempt to transcribe these pieces of information exactly as they appear in the image.'}\n",
      "{'role': 'system', 'content': 'One exception to the previous instruction: this record is from Spanish Florida, which had a substantial amount of cultural crossover with the British colonies to the north, so you can expect to encounter some Anglo surnames in these records.'}\n",
      "{'role': 'system', 'content': 'These records frequently contain idiosyncratic and/or archaic capitalization, punctuation, and word spellings. As you transcribe, amend these artifacts if and only if they affect the clarity of the transcription.'}\n",
      "{'role': 'system', 'content': 'These records also frequently contain abbreviated words or names. Where possible, replace all abbreviations with the full words or names that they represent.'}\n",
      "{'role': 'system', 'content': \"Transcribing these images can be challenging, and certainty is often impossible. When you're uncertain about a name, date, or other piece of important information, please enclose the uncertain transcription in braces {like this}.\"}\n",
      "{'role': 'user', 'content': [{'type': 'text', 'text': 'Please transcribe the sacramental record 0027-02, which appears in the attached images.'}, {'type': 'image_url', 'image_url': {'url': 'https://ssda-openai-test.s3.amazonaws.com/239746-0027-02-color.jpg', 'detail': 'high'}}, {'type': 'image_url', 'image_url': {'url': 'https://ssda-openai-test.s3.amazonaws.com/239746-0027-02-pooled.jpg', 'detail': 'high'}}]}\n",
      "{'role': 'assistant', 'content': '{\"id\": \"0027-02\", \"lines\": [\"Miercoles, dia veinte y uno de Mayo de Mil, sete\", \"-cientos, noventa y quatro. Yo Don Thomas Hassett, Cura\", \"Beneficiado, Vicario, y Juez Eclesi\\\\u00e1stico de esta Iglesia\", \"parroquial y provincia de San Agustin de la Flori\", \"-da Oriental bautic\\\\u00e9 y puse los santos oleos \\\\u00e1 un\", \"ni\\\\u00f1o de color moreno de cinco a\\\\u00f1os de edad, hijo natural\", \"de los negros Ricardo, y Carlota Macqueen, natura\", \"-les de la America del Norte, y todos Esclavos de Don\", \"Juan Macqueen; y en el exerci las sacras ceremo\", \"nias y preces de nuestra santa madre Iglesia, y\", \"le pus\\\\u00e9 por nombre Andres. Fue su padrino el\", \"Doctor Don Thomas Sterling, vecino de esta pro\", \"vincia. Le adverti el parentesco espiritual, y de\", \"mas obligaciones, y lo firm\\\\u00e9 en dicho dia, mes, y\"]}'}\n"
     ]
    }
   ],
   "source": [
    "data_path = \"fine_tune.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17903d61",
   "metadata": {},
   "source": [
    "## Format validation\n",
    "\n",
    "We can perform a variety of error checks to validate that each conversation in the dataset adheres to the format expected by the fine-tuning API. Errors are categorized based on their nature for easier debugging.\n",
    "\n",
    "1. **Data Type Check**: Checks whether each entry in the dataset is a dictionary (`dict`). Error type: `data_type`.\n",
    "2. **Presence of Message List**: Checks if a `messages` list is present in each entry. Error type: `missing_messages_list`.\n",
    "3. **Message Keys Check**: Validates that each message in the `messages` list contains the keys `role` and `content`. Error type: `message_missing_key`.\n",
    "4. **Unrecognized Keys in Messages**: Logs if a message has keys other than `role`, `content`, `weight`, `function_call`, and `name`. Error type: `message_unrecognized_key`.\n",
    "5. **Role Validation**: Ensures the `role` is one of \"system\", \"user\", or \"assistant\". Error type: `unrecognized_role`.\n",
    "6. **Content Validation**: Verifies that `content` has textual data and is a string. Error type: `missing_content`.\n",
    "7. **Assistant Message Presence**: Checks that each conversation has at least one message from the assistant. Error type: `example_missing_assistant_message`.\n",
    "\n",
    "The code below performs these checks, and outputs counts for each type of error found are printed. This is useful for debugging and ensuring the dataset is ready for the next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f3ccbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found errors:\n",
      "missing_content: 8\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "981e77da",
   "metadata": {},
   "source": [
    "## Token Counting Utilities\n",
    "\n",
    "Lets define a few helpful utilities to be used in the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f4b47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fdff67d",
   "metadata": {},
   "source": [
    "## Data Warnings and Token Counts \n",
    "\n",
    "With some lightweight analysis we can identify potential issues in the dataset, like missing messages, and provide statistical insights into message and token counts.\n",
    "\n",
    "1. **Missing System/User Messages**: Counts the number of conversations missing a \"system\" or \"user\" message. Such messages are critical for defining the assistant's behavior and initiating the conversation.\n",
    "2. **Number of Messages Per Example**: Summarizes the distribution of the number of messages in each conversation, providing insight into dialogue complexity.\n",
    "3. **Total Tokens Per Example**: Calculates and summarizes the distribution of the total number of tokens in each conversation. Important for understanding fine-tuning costs.\n",
    "4. **Tokens in Assistant's Messages**: Calculates the number of tokens in the assistant's messages per conversation and summarizes this distribution. Useful for understanding the assistant's verbosity.\n",
    "5. **Token Limit Warnings**: Checks if any examples exceed the maximum token limit (16,385 tokens), as such examples will be truncated during fine-tuning, potentially resulting in data loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52e58ee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m         n_missing_user \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     14\u001b[0m     n_messages\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(messages))\n\u001b[1;32m---> 15\u001b[0m     convo_lens\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnum_tokens_from_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     16\u001b[0m     assistant_message_lens\u001b[38;5;241m.\u001b[39mappend(num_assistant_tokens_from_messages(messages))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNum examples missing system message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_missing_system)\n",
      "Cell \u001b[1;32mIn [5], line 10\u001b[0m, in \u001b[0;36mnum_tokens_from_messages\u001b[1;34m(messages, tokens_per_message, tokens_per_name)\u001b[0m\n\u001b[0;32m      8\u001b[0m num_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tokens_per_message\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 10\u001b[0m     num_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     12\u001b[0m         num_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tokens_per_name\n",
      "File \u001b[1;32mc:\\Users\\genkindn\\AppData\\Local\\miniconda3\\Lib\\site-packages\\tiktoken\\core.py:116\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[1;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(disallowed_special, \u001b[38;5;28mfrozenset\u001b[39m):\n\u001b[0;32m    115\u001b[0m         disallowed_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(disallowed_special)\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;241m:=\u001b[39m \u001b[43m_special_token_regex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisallowed_special\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    117\u001b[0m         raise_disallowed_special_token(match\u001b[38;5;241m.\u001b[39mgroup())\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2afb04df",
   "metadata": {},
   "source": [
    "## Cost Estimation\n",
    "\n",
    "In this final section, we estimate the total number of tokens that will be used for fine-tuning, which allows us to approximate the cost. It is worth noting that the duration of the fine-tuning jobs will also increase with the token count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb95a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~4306 tokens that will be charged for during training\n",
      "By default, you'll train for 20 epochs on this dataset\n",
      "By default, you'll be charged for ~86120 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0ad0369",
   "metadata": {},
   "source": [
    "See https://openai.com/pricing to estimate total costs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
